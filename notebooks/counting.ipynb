{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-10T08:34:28.587265500Z",
     "start_time": "2024-02-10T08:34:22.933870300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from mango.models.diarization import WhisperBasedEENDConfig, WhisperBasedEEND, InternalEENDConfig, MangoEEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1cbdf164f8e45f9ba38d6e8df366603"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e604194f35be48448facf12c550c326f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperEncoder were not initialized from the model checkpoint at openai/whisper-base and are newly initialized: ['model.layers.4.fc1.bias', 'model.layers.5.final_layer_norm.bias', 'model.layers.3.self_attn_layer_norm.bias', 'model.layers.1.final_layer_norm.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.3.final_layer_norm.weight', 'model.layers.0.fc1.bias', 'model.layers.3.self_attn.out_proj.weight', 'model.layers.5.fc2.bias', 'model.layers.5.self_attn.out_proj.weight', 'model.layers.0.self_attn_layer_norm.weight', 'model.layers.5.fc1.weight', 'model.layers.1.self_attn.out_proj.bias', 'model.layers.5.fc2.weight', 'model.conv2.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.4.self_attn_layer_norm.bias', 'model.layers.2.fc1.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.5.final_layer_norm.weight', 'model.layers.1.self_attn.out_proj.weight', 'model.layers.1.self_attn_layer_norm.weight', 'model.layers.0.self_attn_layer_norm.bias', 'model.layers.2.self_attn_layer_norm.bias', 'model.conv1.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.4.self_attn.out_proj.bias', 'model.layer_norm.bias', 'model.layers.2.self_attn.out_proj.bias', 'model.layers.5.self_attn_layer_norm.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.fc2.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.3.fc2.bias', 'model.layers.5.self_attn.out_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.2.final_layer_norm.weight', 'model.layers.0.fc2.weight', 'model.layers.1.fc1.weight', 'model.layers.4.fc2.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.fc2.weight', 'model.layers.1.self_attn_layer_norm.bias', 'model.layers.4.final_layer_norm.bias', 'model.layers.0.final_layer_norm.weight', 'model.layers.2.fc2.weight', 'model.layers.4.self_attn.out_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.conv1.weight', 'model.layer_norm.weight', 'model.layers.0.self_attn.out_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.1.final_layer_norm.bias', 'model.layers.3.self_attn_layer_norm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.2.self_attn_layer_norm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.final_layer_norm.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.3.final_layer_norm.bias', 'model.layers.2.final_layer_norm.bias', 'model.layers.3.self_attn.out_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.0.fc2.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.fc1.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.fc2.bias', 'model.layers.0.self_attn.out_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.3.fc1.bias', 'model.layers.1.fc1.bias', 'model.layers.3.fc1.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.4.self_attn_layer_norm.weight', 'model.layers.4.fc1.weight', 'model.layers.2.self_attn.out_proj.weight', 'model.layers.4.final_layer_norm.weight', 'model.layers.1.fc2.bias', 'model.layers.5.fc1.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.2.fc2.bias', 'model.conv2.bias', 'model.layers.0.fc1.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.embed_positions.weight', 'model.layers.5.self_attn_layer_norm.bias', 'model.layers.4.self_attn.v_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "20086787"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = WhisperBasedEEND(WhisperBasedEENDConfig(num_speakers=3, whisper_checkpoint='openai/whisper-base'))\n",
    "np.sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T08:38:02.122083600Z",
     "start_time": "2024-02-10T08:36:07.778775600Z"
    }
   },
   "id": "1d4cb503feee3769"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from mango.utils.tokenization import retain_cyrillic\n",
    "\n",
    "cv13 = load_dataset('mozilla-foundation/common_voice_13_0', 'uk', trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T09:06:58.637710Z",
     "start_time": "2024-02-10T09:06:50.261318Z"
    }
   },
   "id": "ba854927b920f031"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/16911 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a6621e8a9c341aca4090655c6db02ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/8377 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "315073a92c1f41f08c7254d079805098"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/8383 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66d6f75208344642bc53fcfd61b2653a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2531 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3297032e7f364a358fb0a53e58c6c65a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def length_computing(example):\n",
    "    return {'length' : len(example['audio']['array']) / example['audio']['sampling_rate']}\n",
    "\n",
    "cv13 = cv13.map(length_computing, remove_columns=cv13['train'].column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T09:16:38.430922500Z",
     "start_time": "2024-02-10T09:07:44.777509300Z"
    }
   },
   "id": "357da52adc406b01"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "s = 0\n",
    "for key in cv13:\n",
    "    if 'length' in cv13[key].features:\n",
    "        s += np.sum(cv13[key]['length'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T09:23:12.376234300Z",
     "start_time": "2024-02-10T09:23:12.300232700Z"
    }
   },
   "id": "ff388e79897a0a2c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "47.037031168981486"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s / 60 / 60"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T09:23:21.073293600Z",
     "start_time": "2024-02-10T09:23:21.051293800Z"
    }
   },
   "id": "3e9caf48871dd59a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:13:26.675433100Z",
     "start_time": "2024-04-01T17:13:20.590477300Z"
    }
   },
   "id": "beabfb9d59bbc2e2"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m extractor \u001B[38;5;241m=\u001B[39m WhisperFeatureExtractor()\n\u001B[1;32m----> 3\u001B[0m \u001B[43mextractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m16_000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m27_000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampling_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16_000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\transformers\\models\\whisper\\feature_extraction_whisper.py:216\u001B[0m, in \u001B[0;36mWhisperFeatureExtractor.__call__\u001B[1;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m     raw_speech \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39masarray([speech], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mT \u001B[38;5;28;01mfor\u001B[39;00m speech \u001B[38;5;129;01min\u001B[39;00m raw_speech]\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(raw_speech, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m--> 216\u001B[0m     raw_speech \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_speech\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(raw_speech, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;129;01mand\u001B[39;00m raw_speech\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;129;01mis\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdtype(np\u001B[38;5;241m.\u001B[39mfloat64):\n\u001B[0;32m    218\u001B[0m     raw_speech \u001B[38;5;241m=\u001B[39m raw_speech\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "\u001B[1;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "extractor = WhisperFeatureExtractor()\n",
    "extractor([torch.randn(16_000), torch.randn(27_000)], padding=True, sampling_rate=16_000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:14:56.008752900Z",
     "start_time": "2024-04-01T17:14:55.932747400Z"
    }
   },
   "id": "bae082b68ac7705e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "795f7fea32f15de7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
